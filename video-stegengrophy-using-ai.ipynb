{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video steganography (Kaggle-ready)\n",
        "\n",
        "- **Inputs**: videos in `/kaggle/input/<your-dataset>/...` (auto-detected)\n",
        "- **Outputs**: saved to `/kaggle/working/` (models + demo files)\n",
        "\n",
        "If no video dataset is attached, the notebook falls back to a small dummy dataset so it still runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-26T16:44:31.593405Z",
          "iopub.status.busy": "2026-02-26T16:44:31.592341Z",
          "iopub.status.idle": "2026-02-26T16:44:35.406964Z",
          "shell.execute_reply": "2026-02-26T16:44:35.405915Z",
          "shell.execute_reply.started": "2026-02-26T16:44:31.593355Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Core\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Utils\n",
        "from tqdm.auto import tqdm\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# Crypto (AES-GCM)\n",
        "try:\n",
        "    from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
        "    from cryptography.hazmat.backends import default_backend\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        \"Missing dependency: cryptography. In Kaggle: Settings â†’ Internet ON, then `pip install cryptography`.\"\n",
        "    ) from e\n",
        "\n",
        "\n",
        "def seed_everything(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CFG:\n",
        "    seed: int = 42\n",
        "\n",
        "    # Video\n",
        "    frames: int = 16\n",
        "    video_size: int = 64  # H=W after resizing\n",
        "\n",
        "    # Cover image for stego\n",
        "    cover_size: int = 256\n",
        "\n",
        "    # Model\n",
        "    latent_dim: int = 256\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 4\n",
        "    ae_epochs: int = 2\n",
        "    stego_epochs: int = 2\n",
        "    lr: float = 1e-3\n",
        "\n",
        "    # Secret bits we ask the stego nets to learn (<= cover_size*cover_size)\n",
        "    secret_bits_len: int = 8192\n",
        "\n",
        "    # Kaggle paths\n",
        "    kaggle_input_root: str = \"/kaggle/input\"\n",
        "    work_dir: str = \"/kaggle/working\"\n",
        "\n",
        "    @property\n",
        "    def models_dir(self) -> str:\n",
        "        return os.path.join(self.work_dir, \"models\")\n",
        "\n",
        "    @property\n",
        "    def out_dir(self) -> str:\n",
        "        return os.path.join(self.work_dir, \"outputs\")\n",
        "\n",
        "\n",
        "def find_video_files(root: str, exts=(\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "    if not root or not os.path.exists(root):\n",
        "        return []\n",
        "    files = []\n",
        "    for ext in exts:\n",
        "        files.extend(glob.glob(os.path.join(root, \"**\", f\"*{ext}\"), recursive=True))\n",
        "    return sorted(files)\n",
        "\n",
        "\n",
        "def autodetect_video_root(input_root: str = \"/kaggle/input\") -> str | None:\n",
        "    if not os.path.exists(input_root):\n",
        "        return None\n",
        "    candidates = [os.path.join(input_root, d) for d in os.listdir(input_root)]\n",
        "    candidates = [d for d in candidates if os.path.isdir(d)]\n",
        "    for d in candidates:\n",
        "        if len(find_video_files(d)) > 0:\n",
        "            return d\n",
        "    return None\n",
        "\n",
        "\n",
        "cfg = CFG()\n",
        "seed_everything(cfg.seed)\n",
        "device = get_device()\n",
        "print(\"Device:\", device)\n",
        "print(\"Models dir:\", cfg.models_dir)\n",
        "print(\"Outputs dir:\", cfg.out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-26T16:44:42.677323Z",
          "iopub.status.busy": "2026-02-26T16:44:42.676883Z",
          "iopub.status.idle": "2026-02-26T16:44:42.684854Z",
          "shell.execute_reply": "2026-02-26T16:44:42.683871Z",
          "shell.execute_reply.started": "2026-02-26T16:44:42.677291Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, max_frames=None, resize_dim=(128, 128)):\n",
        "    \"\"\"\n",
        "    Extracts frames from a video file, resizes them, and normalizes them.\n",
        "    Returns a PyTorch tensor of shape (C, T, H, W) where T is the number of frames.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        if resize_dim is not None:\n",
        "            frame = cv2.resize(frame, resize_dim)\n",
        "\n",
        "        frames.append(frame)\n",
        "        if max_frames is not None and len(frames) >= max_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    if not frames:\n",
        "        raise ValueError(f\"Could not extract any frames from {video_path}\")\n",
        "\n",
        "    frames_np = np.array(frames).astype(np.float32) / 255.0\n",
        "    tensor_frames = torch.from_numpy(frames_np).permute(3, 0, 1, 2)\n",
        "    return tensor_frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def compile_video(frames_tensor, output_path, fps=30):\n",
        "    \"\"\"\n",
        "    Reconstructs a video from a PyTorch tensor of shape (C, T, H, W).\n",
        "    \"\"\"\n",
        "    if frames_tensor.requires_grad:\n",
        "        frames_tensor = frames_tensor.detach()\n",
        "    frames_tensor = frames_tensor.cpu()\n",
        "\n",
        "    frames_np = frames_tensor.permute(1, 2, 3, 0).numpy()\n",
        "    frames_np = np.clip(frames_np * 255.0, 0, 255).astype(np.uint8)\n",
        "\n",
        "    T, H, W, C = frames_np.shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n",
        "\n",
        "    for i in range(T):\n",
        "        frame = cv2.cvtColor(frames_np[i], cv2.COLOR_RGB2BGR)\n",
        "        out.write(frame)\n",
        "\n",
        "    out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Video auto encoder**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class VideoEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, latent_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, 32, 3, 2, 1),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(32, 64, 3, 2, 1),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(64, 128, 3, 2, 1),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(128, 256, 3, 2, 1),\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(4096, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.flatten(self.encoder(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class VideoDecoder(nn.Module):\n",
        "    def __init__(self, out_channels=3, latent_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 4096)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(256, 128, 3, 2, 1, 1),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(128, 64, 3, 2, 1, 1),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 32, 3, 2, 1, 1),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(32, out_channels, 3, 2, 1, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.fc(x).view(-1, 256, 1, 4, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class VideoAutoencoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, latent_dim=256):\n",
        "        super().__init__()\n",
        "        self.encoder = VideoEncoder(in_channels, latent_dim)\n",
        "        self.decoder = VideoDecoder(in_channels, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        return self.decoder(latent), latent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Encryption**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_key():\n",
        "    return os.urandom(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_iv():\n",
        "    return os.urandom(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tensor_to_bytes(tensor):\n",
        "    tensor_np = tensor.cpu().detach().numpy().astype(\"float32\")\n",
        "    return tensor_np.tobytes(), tensor_np.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def bytes_to_tensor(byte_data, shape, device=\"cpu\"):\n",
        "    import numpy as np\n",
        "\n",
        "    tensor_np = np.frombuffer(byte_data, dtype=\"float32\").copy().reshape(shape)\n",
        "    return torch.from_numpy(tensor_np).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def encrypt_data(data_bytes, key, iv):\n",
        "    encryptor = Cipher(\n",
        "        algorithms.AES(key), modes.GCM(iv), backend=default_backend()\n",
        "    ).encryptor()\n",
        "    return encryptor.update(data_bytes) + encryptor.finalize(), encryptor.tag\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def decrypt_data(ciphertext, tag, key, iv):\n",
        "    decryptor = Cipher(\n",
        "        algorithms.AES(key), modes.GCM(iv, tag), backend=default_backend()\n",
        "    ).decryptor()\n",
        "    return decryptor.update(ciphertext) + decryptor.finalize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class LatentEncryptor:\n",
        "    def __init__(self, key=None):\n",
        "        self.key = key if key else generate_key()\n",
        "\n",
        "    def encrypt(self, latent_tensor):\n",
        "        data_bytes, shape = tensor_to_bytes(latent_tensor)\n",
        "        iv = generate_iv()\n",
        "        ciphertext, tag = encrypt_data(data_bytes, self.key, iv)\n",
        "        return ciphertext, {\"iv\": iv, \"tag\": tag, \"shape\": shape}\n",
        "\n",
        "    def decrypt(self, ciphertext, metadata, device=\"cpu\"):\n",
        "        plaintext = decrypt_data(ciphertext, metadata[\"tag\"], self.key, metadata[\"iv\"])\n",
        "        return bytes_to_tensor(plaintext, metadata[\"shape\"], device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def ciphertext_to_bits(ciphertext, max_len=None):\n",
        "    import numpy as np\n",
        "\n",
        "    byte_array = np.frombuffer(ciphertext, dtype=np.uint8)\n",
        "    bit_array = np.unpackbits(byte_array).astype(np.float32)\n",
        "    if max_len is not None:\n",
        "        padded = np.zeros(max_len, dtype=np.float32)\n",
        "        padded[: len(bit_array)] = bit_array\n",
        "        bit_array = padded\n",
        "    return torch.from_numpy(bit_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def bits_to_ciphertext(bit_tensor, original_byte_len):\n",
        "    import numpy as np\n",
        "\n",
        "    bit_array = (bit_tensor.cpu().numpy() >= 0.5).astype(np.uint8)[\n",
        "        : original_byte_len * 8\n",
        "    ]\n",
        "    return np.packbits(bit_array).tobytes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **# Image Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ImageGenerator:\n",
        "    \"\"\"Wrapper for AI image generation.\"\"\"\n",
        "\n",
        "    def __init__(self, device=\"cpu\", use_dummy=True):\n",
        "        self.device = device\n",
        "        self.use_dummy = use_dummy\n",
        "        self.pipeline = None\n",
        "\n",
        "        if not self.use_dummy:\n",
        "            try:\n",
        "                from diffusers import StableDiffusionPipeline\n",
        "\n",
        "                self.pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
        "                )\n",
        "                self.pipeline = self.pipeline.to(self.device)\n",
        "            except ImportError:\n",
        "                print(\"Diffusers not installed. Falling back to dummy generator.\")\n",
        "                self.use_dummy = True\n",
        "\n",
        "    def generate_cover(\n",
        "        self,\n",
        "        prompt=\"A beautiful realistic landscape photo, 4k resolution\",\n",
        "        size=(256, 256),\n",
        "    ):\n",
        "        if self.use_dummy or self.pipeline is None:\n",
        "            img_tensor = (\n",
        "                torch.rand((3, size[0], size[1]), dtype=torch.float32)\n",
        "                .to(self.device)\n",
        "                .unsqueeze(0)\n",
        "            )\n",
        "            import torch.nn.functional as F\n",
        "\n",
        "            img_tensor = F.avg_pool2d(\n",
        "                img_tensor, kernel_size=5, stride=1, padding=2\n",
        "            ).squeeze(0)\n",
        "            return (img_tensor - img_tensor.min()) / (\n",
        "                img_tensor.max() - img_tensor.min() + 1e-8\n",
        "            )\n",
        "        else:\n",
        "            image = self.pipeline(\n",
        "                prompt, height=size[0], width=size[1], num_inference_steps=20\n",
        "            ).images[0]\n",
        "            image_np = np.array(image).astype(np.float32) / 255.0\n",
        "            return torch.from_numpy(image_np).permute(2, 0, 1).to(self.device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Stego Networks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class HiderNetwork(nn.Module):\n",
        "    def __init__(self, cover_channels=3, secret_channels=1, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        in_channels = cover_channels + secret_channels\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, cover_channels, 3, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, cover, secret):\n",
        "        return self.net(torch.cat([cover, secret], dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class RevealerNetwork(nn.Module):\n",
        "    def __init__(self, stego_channels=3, secret_channels=1, hidden_channels=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(stego_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, secret_channels, 3, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, stego):\n",
        "        return self.net(stego)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def format_secret_for_hiding(secret_bits, target_shape):\n",
        "    B, C, H, W = target_shape\n",
        "    total_elements = C * H * W\n",
        "    padded = torch.zeros(B, total_elements, device=secret_bits.device)\n",
        "    for i in range(B):\n",
        "        seq = secret_bits[i] if secret_bits.dim() > 1 else secret_bits\n",
        "        length = min(len(seq), total_elements)\n",
        "        padded[i, :length] = seq[:length]\n",
        "    return padded.view(B, C, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def extract_secret_from_prediction(secret_pred_spatial, original_length):\n",
        "    return secret_pred_spatial.view(secret_pred_spatial.shape[0], -1)[\n",
        "        :, :original_length\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Everything is defined in this notebook (no local .py imports needed on Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class DummyVideoDataset(Dataset):\n",
        "    def __init__(self, num_samples=100, frames=16, height=64, width=64):\n",
        "        self.num_samples = num_samples\n",
        "        self.frames = frames\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.rand(\n",
        "            (3, self.frames, self.height, self.width), dtype=torch.float32\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class RealVideoDataset(Dataset):\n",
        "    \"\"\"Loads actual .mp4 or .avi videos from a directory for autoencoder training.\"\"\"\n",
        "    def __init__(self, directory, frames=16, height=64, width=64):\n",
        "        self.video_paths = glob.glob(os.path.join(directory, \"**\", \"*.avi\"), recursive=True) + \\\n",
        "                           glob.glob(os.path.join(directory, \"**\", \"*.mp4\"), recursive=True)\n",
        "        self.frames = frames\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((self.height, self.width)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cap = cv2.VideoCapture(self.video_paths[idx])\n",
        "        frames = []\n",
        "        while len(frames) < self.frames:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_tensor = self.transform(frame) # shape (3, H, W)\n",
        "            frames.append(frame_tensor)\n",
        "        cap.release()\n",
        "        \n",
        "        # If video is too short, pad it with the last frame\n",
        "        while len(frames) < self.frames and len(frames) > 0:\n",
        "            frames.append(frames[-1])\n",
        "            \n",
        "        # If video couldn't be loaded at all, return zeros (edge case fallback)\n",
        "        if len(frames) == 0:\n",
        "            return torch.zeros((3, self.frames, self.height, self.width), dtype=torch.float32)\n",
        "            \n",
        "        # Stack into (C, F, H, W)\n",
        "        video_tensor = torch.stack(frames, dim=1)\n",
        "        return video_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_video_autoencoder(model, dataloader, *, epochs: int, device, lr: float, save_dir: str):\n",
        "    print(\"--- Training Video Autoencoder ---\")\n",
        "    if dataloader is None or len(dataloader) == 0:\n",
        "        print(\"No data found for autoencoder training; skipping.\")\n",
        "        return\n",
        "\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        pbar = tqdm(dataloader, desc=f\"AE epoch {epoch+1}/{epochs}\", leave=False)\n",
        "        for batch in pbar:\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            reconstructed, _ = model(batch)\n",
        "            loss = criterion(reconstructed, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += loss.item()\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | loss={running/len(dataloader):.6f}\")\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, \"video_autoencoder.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_stego_networks(\n",
        "    hider,\n",
        "    revealer,\n",
        "    image_generator,\n",
        "    *,\n",
        "    epochs: int,\n",
        "    device,\n",
        "    lr: float,\n",
        "    secret_bits_len: int,\n",
        "    cover_size: int,\n",
        "    save_dir: str,\n",
        "    batch_size: int = 4,\n",
        "    iterations_per_epoch: int = 50,\n",
        "    img_weight: float = 10.0,\n",
        "):\n",
        "    print(\"\\n--- Training Steganography Networks ---\")\n",
        "    hider.to(device)\n",
        "    revealer.to(device)\n",
        "\n",
        "    criterion_mse = nn.MSELoss()\n",
        "    criterion_bce = nn.BCELoss()\n",
        "    optimizer = optim.Adam(list(hider.parameters()) + list(revealer.parameters()), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        hider.train()\n",
        "        revealer.train()\n",
        "        img_loss_sum = 0.0\n",
        "        bit_loss_sum = 0.0\n",
        "\n",
        "        for _ in tqdm(range(iterations_per_epoch), desc=f\"Stego epoch {epoch+1}/{epochs}\", leave=False):\n",
        "            covers = torch.stack(\n",
        "                [image_generator.generate_cover(size=(cover_size, cover_size)) for _ in range(batch_size)]\n",
        "            ).to(device)\n",
        "\n",
        "            secret_bits = torch.randint(0, 2, (batch_size, secret_bits_len), device=device).float()\n",
        "            spatial_secret = format_secret_for_hiding(secret_bits, (batch_size, 1, cover_size, cover_size))\n",
        "\n",
        "            stego = hider(covers, spatial_secret)\n",
        "            secret_pred = extract_secret_from_prediction(revealer(stego), secret_bits_len)\n",
        "            secret_pred = torch.clamp(secret_pred, 1e-6, 1.0 - 1e-6)\n",
        "\n",
        "            l_img = criterion_mse(stego, covers)\n",
        "            l_bit = criterion_bce(secret_pred, secret_bits)\n",
        "            loss = (img_weight * l_img) + l_bit\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            img_loss_sum += l_img.item()\n",
        "            bit_loss_sum += l_bit.item()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}/{epochs} | img_loss={img_loss_sum/iterations_per_epoch:.6f} | bit_loss={bit_loss_sum/iterations_per_epoch:.6f}\"\n",
        "        )\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    torch.save(hider.state_dict(), os.path.join(save_dir, \"hider.pth\"))\n",
        "    torch.save(revealer.state_dict(), os.path.join(save_dir, \"revealer.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Kaggle: train models\n",
        "# =============================\n",
        "\n",
        "os.makedirs(cfg.models_dir, exist_ok=True)\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "video_root = autodetect_video_root(cfg.kaggle_input_root)\n",
        "video_files = find_video_files(video_root) if video_root else []\n",
        "\n",
        "if len(video_files) > 0:\n",
        "    print(f\"Found {len(video_files)} videos under: {video_root}\")\n",
        "    video_dataset = RealVideoDataset(\n",
        "        directory=video_root,\n",
        "        frames=cfg.frames,\n",
        "        height=cfg.video_size,\n",
        "        width=cfg.video_size,\n",
        "    )\n",
        "else:\n",
        "    print(\"No videos found in /kaggle/input. Using a dummy dataset so the notebook runs.\")\n",
        "    video_dataset = DummyVideoDataset(\n",
        "        num_samples=32,\n",
        "        frames=cfg.frames,\n",
        "        height=cfg.video_size,\n",
        "        width=cfg.video_size,\n",
        "    )\n",
        "\n",
        "pin = device.type == \"cuda\"\n",
        "video_loader = DataLoader(\n",
        "    video_dataset,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=pin,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "# 1) Video autoencoder\n",
        "video_ae = VideoAutoencoder(in_channels=3, latent_dim=cfg.latent_dim)\n",
        "train_video_autoencoder(\n",
        "    video_ae,\n",
        "    video_loader,\n",
        "    epochs=cfg.ae_epochs,\n",
        "    device=device,\n",
        "    lr=cfg.lr,\n",
        "    save_dir=cfg.models_dir,\n",
        ")\n",
        "\n",
        "# 2) Stego networks (cover images are generated; keep dummy generator for Kaggle stability)\n",
        "hider = HiderNetwork(cover_channels=3, secret_channels=1, hidden_channels=32)\n",
        "revealer = RevealerNetwork(stego_channels=3, secret_channels=1, hidden_channels=32)\n",
        "img_gen = ImageGenerator(device=device, use_dummy=True)\n",
        "\n",
        "train_stego_networks(\n",
        "    hider,\n",
        "    revealer,\n",
        "    img_gen,\n",
        "    epochs=cfg.stego_epochs,\n",
        "    device=device,\n",
        "    lr=cfg.lr,\n",
        "    secret_bits_len=cfg.secret_bits_len,\n",
        "    cover_size=cfg.cover_size,\n",
        "    save_dir=cfg.models_dir,\n",
        "    batch_size=cfg.batch_size,\n",
        ")\n",
        "\n",
        "print(\"Training complete. Checkpoints saved to:\", cfg.models_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# (imports moved to the top Kaggle setup cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Everything is defined in this notebook (no local .py imports needed on Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def save_image(tensor, path):\n",
        "    import torchvision\n",
        "\n",
        "    torchvision.utils.save_image(tensor, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class SteganoPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        device,\n",
        "        models_dir: str,\n",
        "        frames: int,\n",
        "        video_size: int,\n",
        "        cover_size: int,\n",
        "        latent_dim: int,\n",
        "        use_dummy_covers: bool = True,\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.frames = frames\n",
        "        self.video_size = video_size\n",
        "        self.cover_size = cover_size\n",
        "\n",
        "        self.autoencoder = VideoAutoencoder(in_channels=3, latent_dim=latent_dim).to(device)\n",
        "        self.hider = HiderNetwork(cover_channels=3, secret_channels=1).to(device)\n",
        "        self.revealer = RevealerNetwork(stego_channels=3, secret_channels=1).to(device)\n",
        "\n",
        "        # Runtime-only secret key (keep same object for hide+extract)\n",
        "        self.encryptor = LatentEncryptor()\n",
        "\n",
        "        # Covers (keep dummy by default for Kaggle reliability)\n",
        "        self.generator = ImageGenerator(device=device, use_dummy=use_dummy_covers)\n",
        "\n",
        "        # Load checkpoints if present\n",
        "        ae_ckpt = os.path.join(models_dir, \"video_autoencoder.pth\")\n",
        "        hider_ckpt = os.path.join(models_dir, \"hider.pth\")\n",
        "        revealer_ckpt = os.path.join(models_dir, \"revealer.pth\")\n",
        "\n",
        "        if os.path.exists(ae_ckpt):\n",
        "            self.autoencoder.load_state_dict(torch.load(ae_ckpt, map_location=device))\n",
        "        if os.path.exists(hider_ckpt):\n",
        "            self.hider.load_state_dict(torch.load(hider_ckpt, map_location=device))\n",
        "        if os.path.exists(revealer_ckpt):\n",
        "            self.revealer.load_state_dict(torch.load(revealer_ckpt, map_location=device))\n",
        "\n",
        "        self.autoencoder.eval()\n",
        "        self.hider.eval()\n",
        "        self.revealer.eval()\n",
        "\n",
        "    def hide_video(self, video_path: str, output_image_path: str):\n",
        "        print(f\"1. Extracting frames from {video_path}...\")\n",
        "        frames = extract_frames(\n",
        "            video_path,\n",
        "            max_frames=self.frames,\n",
        "            resize_dim=(self.video_size, self.video_size),\n",
        "        ).unsqueeze(0).to(self.device)\n",
        "\n",
        "        print(\"2. Compressing video into latent vector...\")\n",
        "        with torch.no_grad():\n",
        "            _, latent = self.autoencoder(frames)\n",
        "\n",
        "        print(\"3. Encrypting latent vector using AES-GCM...\")\n",
        "        ciphertext, metadata = self.encryptor.encrypt(latent[0])\n",
        "\n",
        "        print(f\"4. Generating cover image ({self.cover_size}x{self.cover_size})...\")\n",
        "        cover_image = self.generator.generate_cover(size=(self.cover_size, self.cover_size)).unsqueeze(0).to(self.device)\n",
        "\n",
        "        print(\"5. Packing encrypted data into spatial tensor...\")\n",
        "        max_bits = self.cover_size * self.cover_size\n",
        "        spatial_secret = format_secret_for_hiding(\n",
        "            ciphertext_to_bits(ciphertext, max_bits).unsqueeze(0).to(self.device),\n",
        "            (1, 1, self.cover_size, self.cover_size),\n",
        "        )\n",
        "\n",
        "        print(\"6. Embedding secret into cover image...\")\n",
        "        with torch.no_grad():\n",
        "            stego_image = self.hider(cover_image, spatial_secret)\n",
        "\n",
        "        print(f\"7. Saving stego image to {output_image_path}...\")\n",
        "        save_image(stego_image[0], output_image_path)\n",
        "        return metadata, len(ciphertext)\n",
        "\n",
        "    def extract_video(self, stego_image_path: str, metadata, cipher_len: int, output_video_path: str):\n",
        "        from torchvision.io import read_image\n",
        "\n",
        "        print(f\"1. Loading stego image from {stego_image_path}...\")\n",
        "        stego_image = (read_image(stego_image_path).float() / 255.0).unsqueeze(0).to(self.device)\n",
        "\n",
        "        print(\"2. Extracting spatial data...\")\n",
        "        with torch.no_grad():\n",
        "            secret_pred_spatial = self.revealer(stego_image)\n",
        "\n",
        "        print(\"3. Reconstructing bit stream...\")\n",
        "        bit_tensor = extract_secret_from_prediction(secret_pred_spatial, cipher_len * 8)\n",
        "\n",
        "        print(\"4. Repacking bits to ciphertext...\")\n",
        "        recovered_ciphertext = bits_to_ciphertext(bit_tensor[0], cipher_len)\n",
        "\n",
        "        print(\"5. Decrypting latent vector using AES-GCM...\")\n",
        "        recovered_latent = self.encryptor.decrypt(recovered_ciphertext, metadata, device=self.device)\n",
        "\n",
        "        print(\"6. Reconstructing video frames...\")\n",
        "        with torch.no_grad():\n",
        "            reconstructed_frames = self.autoencoder.decoder(recovered_latent.unsqueeze(0))\n",
        "\n",
        "        print(f\"7. Saving reconstructed video to {output_video_path}...\")\n",
        "        compile_video(reconstructed_frames[0], output_video_path, fps=15)\n",
        "        print(\"--- Decode complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Kaggle: quick end-to-end demo\n",
        "# =============================\n",
        "\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "# Use a real video if available, else create a tiny dummy\n",
        "video_root = autodetect_video_root(cfg.kaggle_input_root)\n",
        "video_files = find_video_files(video_root) if video_root else []\n",
        "\n",
        "if len(video_files) > 0:\n",
        "    demo_video_path = video_files[0]\n",
        "    print(\"Using demo video:\", demo_video_path)\n",
        "else:\n",
        "    demo_video_path = os.path.join(cfg.out_dir, \"dummy_video.mp4\")\n",
        "    print(\"No input videos found; writing dummy video:\", demo_video_path)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(demo_video_path, fourcc, 15, (cfg.video_size, cfg.video_size))\n",
        "    for _ in range(cfg.frames):\n",
        "        out.write(np.random.randint(0, 255, (cfg.video_size, cfg.video_size, 3), dtype=np.uint8))\n",
        "    out.release()\n",
        "\n",
        "pipeline = SteganoPipeline(\n",
        "    device=device,\n",
        "    models_dir=cfg.models_dir,\n",
        "    frames=cfg.frames,\n",
        "    video_size=cfg.video_size,\n",
        "    cover_size=cfg.cover_size,\n",
        "    latent_dim=cfg.latent_dim,\n",
        "    use_dummy_covers=True,\n",
        ")\n",
        "\n",
        "stego_img_path = os.path.join(cfg.out_dir, \"stego_output.png\")\n",
        "recon_video_path = os.path.join(cfg.out_dir, \"reconstructed_video.mp4\")\n",
        "\n",
        "metadata, cipher_length = pipeline.hide_video(demo_video_path, stego_img_path)\n",
        "pipeline.extract_video(stego_img_path, metadata, cipher_length, recon_video_path)\n",
        "\n",
        "print(\"Demo outputs written to:\", cfg.out_dir)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31286,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
