{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31286,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Utils**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T16:44:31.592341Z","iopub.execute_input":"2026-02-26T16:44:31.593405Z","iopub.status.idle":"2026-02-26T16:44:35.406964Z","shell.execute_reply.started":"2026-02-26T16:44:31.593355Z","shell.execute_reply":"2026-02-26T16:44:35.405915Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def extract_frames(video_path, max_frames=None, resize_dim=(128, 128)):\n    \"\"\"\n    Extracts frames from a video file, resizes them, and normalizes them.\n    Returns a PyTorch tensor of shape (C, T, H, W) where T is the number of frames.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        if resize_dim is not None:\n            frame = cv2.resize(frame, resize_dim)\n\n        frames.append(frame)\n        if max_frames is not None and len(frames) >= max_frames:\n            break\n\n    cap.release()\n    if not frames:\n        raise ValueError(f\"Could not extract any frames from {video_path}\")\n\n    frames_np = np.array(frames).astype(np.float32) / 255.0\n    tensor_frames = torch.from_numpy(frames_np).permute(3, 0, 1, 2)\n    return tensor_frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-26T16:44:42.676883Z","iopub.execute_input":"2026-02-26T16:44:42.677323Z","iopub.status.idle":"2026-02-26T16:44:42.684854Z","shell.execute_reply.started":"2026-02-26T16:44:42.677291Z","shell.execute_reply":"2026-02-26T16:44:42.683871Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def compile_video(frames_tensor, output_path, fps=30):\n    \"\"\"\n    Reconstructs a video from a PyTorch tensor of shape (C, T, H, W).\n    \"\"\"\n    if frames_tensor.requires_grad:\n        frames_tensor = frames_tensor.detach()\n    frames_tensor = frames_tensor.cpu()\n\n    frames_np = frames_tensor.permute(1, 2, 3, 0).numpy()\n    frames_np = np.clip(frames_np * 255.0, 0, 255).astype(np.uint8)\n\n    T, H, W, C = frames_np.shape\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n\n    for i in range(T):\n        frame = cv2.cvtColor(frames_np[i], cv2.COLOR_RGB2BGR)\n        out.write(frame)\n\n    out.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Video auto encoder**\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoEncoder(nn.Module):\n    def __init__(self, in_channels=3, latent_dim=256):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv3d(in_channels, 32, 3, 2, 1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(),\n            nn.Conv3d(32, 64, 3, 2, 1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(),\n            nn.Conv3d(64, 128, 3, 2, 1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(),\n            nn.Conv3d(128, 256, 3, 2, 1),\n            nn.BatchNorm3d(256),\n            nn.ReLU(),\n        )\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(4096, latent_dim)\n\n    def forward(self, x):\n        return self.fc(self.flatten(self.encoder(x)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoDecoder(nn.Module):\n    def __init__(self, out_channels=3, latent_dim=256):\n        super().__init__()\n        self.fc = nn.Linear(latent_dim, 4096)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose3d(256, 128, 3, 2, 1, 1),\n            nn.BatchNorm3d(128),\n            nn.ReLU(),\n            nn.ConvTranspose3d(128, 64, 3, 2, 1, 1),\n            nn.BatchNorm3d(64),\n            nn.ReLU(),\n            nn.ConvTranspose3d(64, 32, 3, 2, 1, 1),\n            nn.BatchNorm3d(32),\n            nn.ReLU(),\n            nn.ConvTranspose3d(32, out_channels, 3, 2, 1, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.decoder(self.fc(x).view(-1, 256, 1, 4, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoAutoencoder(nn.Module):\n    def __init__(self, in_channels=3, latent_dim=256):\n        super().__init__()\n        self.encoder = VideoEncoder(in_channels, latent_dim)\n        self.decoder = VideoDecoder(in_channels, latent_dim)\n\n    def forward(self, x):\n        latent = self.encoder(x)\n        return self.decoder(latent), latent","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Encryption**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\nfrom cryptography.hazmat.backends import default_backend","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_key():\n    return os.urandom(32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_iv():\n    return os.urandom(12)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tensor_to_bytes(tensor):\n    tensor_np = tensor.cpu().detach().numpy().astype(\"float32\")\n    return tensor_np.tobytes(), tensor_np.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bytes_to_tensor(byte_data, shape, device=\"cpu\"):\n    import numpy as np\n\n    tensor_np = np.frombuffer(byte_data, dtype=\"float32\").copy().reshape(shape)\n    return torch.from_numpy(tensor_np).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encrypt_data(data_bytes, key, iv):\n    encryptor = Cipher(\n        algorithms.AES(key), modes.GCM(iv), backend=default_backend()\n    ).encryptor()\n    return encryptor.update(data_bytes) + encryptor.finalize(), encryptor.tag\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decrypt_data(ciphertext, tag, key, iv):\n    decryptor = Cipher(\n        algorithms.AES(key), modes.GCM(iv, tag), backend=default_backend()\n    ).decryptor()\n    return decryptor.update(ciphertext) + decryptor.finalize()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LatentEncryptor:\n    def __init__(self, key=None):\n        self.key = key if key else generate_key()\n\n    def encrypt(self, latent_tensor):\n        data_bytes, shape = tensor_to_bytes(latent_tensor)\n        iv = generate_iv()\n        ciphertext, tag = encrypt_data(data_bytes, self.key, iv)\n        return ciphertext, {\"iv\": iv, \"tag\": tag, \"shape\": shape}\n\n    def decrypt(self, ciphertext, metadata, device=\"cpu\"):\n        plaintext = decrypt_data(ciphertext, metadata[\"tag\"], self.key, metadata[\"iv\"])\n        return bytes_to_tensor(plaintext, metadata[\"shape\"], device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ciphertext_to_bits(ciphertext, max_len=None):\n    import numpy as np\n\n    byte_array = np.frombuffer(ciphertext, dtype=np.uint8)\n    bit_array = np.unpackbits(byte_array).astype(np.float32)\n    if max_len is not None:\n        padded = np.zeros(max_len, dtype=np.float32)\n        padded[: len(bit_array)] = bit_array\n        bit_array = padded\n    return torch.from_numpy(bit_array)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def bits_to_ciphertext(bit_tensor, original_byte_len):\n    import numpy as np\n\n    bit_array = (bit_tensor.cpu().numpy() >= 0.5).astype(np.uint8)[\n        : original_byte_len * 8\n    ]\n    return np.packbits(bit_array).tobytes()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **# Image Generation**","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageGenerator:\n    \"\"\"Wrapper for AI image generation.\"\"\"\n\n    def __init__(self, device=\"cpu\", use_dummy=True):\n        self.device = device\n        self.use_dummy = use_dummy\n        self.pipeline = None\n\n        if not self.use_dummy:\n            try:\n                from diffusers import StableDiffusionPipeline\n\n                self.pipeline = StableDiffusionPipeline.from_pretrained(\n                    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n                )\n                self.pipeline = self.pipeline.to(self.device)\n            except ImportError:\n                print(\"Diffusers not installed. Falling back to dummy generator.\")\n                self.use_dummy = True\n\n    def generate_cover(\n        self,\n        prompt=\"A beautiful realistic landscape photo, 4k resolution\",\n        size=(256, 256),\n    ):\n        if self.use_dummy or self.pipeline is None:\n            img_tensor = (\n                torch.rand((3, size[0], size[1]), dtype=torch.float32)\n                .to(self.device)\n                .unsqueeze(0)\n            )\n            import torch.nn.functional as F\n\n            img_tensor = F.avg_pool2d(\n                img_tensor, kernel_size=5, stride=1, padding=2\n            ).squeeze(0)\n            return (img_tensor - img_tensor.min()) / (\n                img_tensor.max() - img_tensor.min() + 1e-8\n            )\n        else:\n            image = self.pipeline(\n                prompt, height=size[0], width=size[1], num_inference_steps=20\n            ).images[0]\n            image_np = np.array(image).astype(np.float32) / 255.0\n            return torch.from_numpy(image_np).permute(2, 0, 1).to(self.device)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Stego Networks**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HiderNetwork(nn.Module):\n    def __init__(self, cover_channels=3, secret_channels=1, hidden_channels=64):\n        super().__init__()\n        in_channels = cover_channels + secret_channels\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, cover_channels, 3, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, cover, secret):\n        return self.net(torch.cat([cover, secret], dim=1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RevealerNetwork(nn.Module):\n    def __init__(self, stego_channels=3, secret_channels=1, hidden_channels=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(stego_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, hidden_channels, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_channels, secret_channels, 3, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, stego):\n        return self.net(stego)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_secret_for_hiding(secret_bits, target_shape):\n    B, C, H, W = target_shape\n    total_elements = C * H * W\n    padded = torch.zeros(B, total_elements, device=secret_bits.device)\n    for i in range(B):\n        seq = secret_bits[i] if secret_bits.dim() > 1 else secret_bits\n        length = min(len(seq), total_elements)\n        padded[i, :length] = seq[:length]\n    return padded.view(B, C, H, W)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_secret_from_prediction(secret_pred_spatial, original_length):\n    return secret_pred_spatial.view(secret_pred_spatial.shape[0], -1)[\n        :, :original_length\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from video_autoencoder import VideoAutoencoder\nfrom stego_networks import (\n    HiderNetwork,\n    RevealerNetwork,\n    format_secret_for_hiding,\n    extract_secret_from_prediction,\n)\nfrom image_generator import ImageGenerator","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DummyVideoDataset(Dataset):\n    def __init__(self, num_samples=100, frames=16, height=64, width=64):\n        self.num_samples = num_samples\n        self.frames = frames\n        self.height = height\n        self.width = width\n\n    def __len__(self):\n        return self.num_samples\n\n    def __getitem__(self, idx):\n        return torch.rand(\n            (3, self.frames, self.height, self.width), dtype=torch.float32\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport glob\nfrom torchvision import transforms\nfrom PIL import Image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class RealVideoDataset(Dataset):\n    \"\"\"Loads actual .mp4 or .avi videos from a directory for autoencoder training.\"\"\"\n    def __init__(self, directory, frames=16, height=64, width=64):\n        self.video_paths = glob.glob(os.path.join(directory, \"**\", \"*.avi\"), recursive=True) + \\\n                           glob.glob(os.path.join(directory, \"**\", \"*.mp4\"), recursive=True)\n        self.frames = frames\n        self.height = height\n        self.width = width\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((self.height, self.width)),\n            transforms.ToTensor()\n        ])\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        cap = cv2.VideoCapture(self.video_paths[idx])\n        frames = []\n        while len(frames) < self.frames:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_tensor = self.transform(frame) # shape (3, H, W)\n            frames.append(frame_tensor)\n        cap.release()\n        \n        # If video is too short, pad it with the last frame\n        while len(frames) < self.frames and len(frames) > 0:\n            frames.append(frames[-1])\n            \n        # If video couldn't be loaded at all, return zeros (edge case fallback)\n        if len(frames) == 0:\n            return torch.zeros((3, self.frames, self.height, self.width), dtype=torch.float32)\n            \n        # Stack into (C, F, H, W)\n        video_tensor = torch.stack(frames, dim=1)\n        return video_tensor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_video_autoencoder(model, dataloader, epochs=5, device=\"cpu\"):\n    print(\"--- Training Video Autoencoder ---\")\n    model.to(device)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0.0\n        for batch in dataloader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            reconstructed, _ = model(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n    os.makedirs(\"../models\", exist_ok=True)\n    torch.save(model.state_dict(), \"../models/video_autoencoder.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_stego_networks(\n    hider, revealer, image_generator, epochs=5, device=\"cpu\", secret_dim=4096\n):\n    print(\"\\\\n--- Training Steganography Networks ---\")\n    hider.to(device)\n    revealer.to(device)\n    criterion_mse = nn.MSELoss()\n    criterion_bce = nn.BCELoss()\n    optimizer = optim.Adam(\n        list(hider.parameters()) + list(revealer.parameters()), lr=1e-3\n    )\n    batch_size = 4\n    iterations = 20\n    for epoch in range(epochs):\n        hider.train()\n        revealer.train()\n        img_loss = 0.0\n        bit_loss = 0.0\n        for _ in range(iterations):\n            covers = torch.stack(\n                [image_generator.generate_cover(size=(256, 256)) for _ in range(batch_size)]\n            ).to(device)\n            secret_bits = (\n                torch.randint(0, 2, (batch_size, secret_dim)).float().to(device)\n            )\n            spatial_secret = format_secret_for_hiding(\n                secret_bits, (batch_size, 1, 256, 256)\n            )\n            stego = hider(covers, spatial_secret)\n            secret_pred = extract_secret_from_prediction(revealer(stego), secret_dim)\n            secret_pred = torch.clamp(secret_pred, 1e-7, 1.0 - 1e-7)\n            l_img = criterion_mse(stego, covers)\n            l_bit = criterion_bce(secret_pred, secret_bits)\n            loss = (10.0 * l_img) + l_bit\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            img_loss += l_img.item()\n            bit_loss += l_bit.item()\n        print(\n            f\"Epoch {epoch+1}/{epochs}, Img Loss: {img_loss/iterations:.4f}, Bit Loss: {bit_loss/iterations:.4f}\"\n        )\n    torch.save(hider.state_dict(), \"../models/hider.pth\")\n    torch.save(revealer.state_dict(), \"../models/revealer.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        device = torch.device(\"mps\") # Uses Apple Silicon GPU!\n    else:\n        device = torch.device(\"cpu\")\n        \n    print(f\"Using device: {device}\")\n    \n    # -------------------------------------------------------------\n    # 1. SETUP: CHOOSE DUMMY DATA OR REAL DATA\n    # -------------------------------------------------------------\n    # Change this to True when you have downloaded the datasets!\n    USE_REAL_DATA = True \n    \n    # Define paths to your downloaded folders\n    VIDEO_DATASET_PATH = \"dataset\" # Update this path on Kaggle!\n    \n    if USE_REAL_DATA:\n        print(\"Loading REAL Video Dataset... (This might take a moment)\")\n        # Load your real video dataset and create a dataloader\n        video_dataset = RealVideoDataset(directory=VIDEO_DATASET_PATH, frames=16)\n        video_loader = DataLoader(video_dataset, batch_size=4, shuffle=True)\n    else:\n        print(\"Loading DUMMY Video Dataset... (For quick testing)\")\n        video_dataset = DummyVideoDataset(num_samples=20)\n        video_loader = DataLoader(video_dataset, batch_size=4)\n\n    # -------------------------------------------------------------\n    # 2. RUN AUTOENCODER TRAINING\n    # -------------------------------------------------------------\n    ae = VideoAutoencoder(3, 256)\n    train_video_autoencoder(ae, video_loader, epochs=2, device=device)\n\n    # -------------------------------------------------------------\n    # 3. RUN STEGANOGRAPHY NETWORKS TRAINING\n    # -------------------------------------------------------------\n    # For Stego networks, Stable Diffusion `ImageGenerator(use_dummy=not USE_REAL_DATA)` \n    # will handle real cover images if USE_REAL_DATA=True!\n    hider = HiderNetwork(3, 1, 32)\n    revealer = RevealerNetwork(3, 1, 32)\n    img_gen = ImageGenerator(device, use_dummy=not USE_REAL_DATA)\n    \n    train_stego_networks(\n        hider,\n        revealer,\n        img_gen,\n        epochs=2,\n        device=device,\n        secret_dim=8416, \n    )\n    print(\"Training Complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pipeline**","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport cv2\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from utils import extract_frames, compile_video\nfrom video_autoencoder import VideoAutoencoder\nfrom encryption import LatentEncryptor, ciphertext_to_bits, bits_to_ciphertext\nfrom image_generator import ImageGenerator\nfrom stego_networks import (\n    HiderNetwork,\n    RevealerNetwork,\n    format_secret_for_hiding,\n    extract_secret_from_prediction,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_image(tensor, path):\n    import torchvision\n\n    torchvision.utils.save_image(tensor, path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SteganoPipeline:\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n        self.autoencoder = VideoAutoencoder(in_channels=3, latent_dim=256).to(device)\n        self.encryptor = LatentEncryptor()\n        self.generator = ImageGenerator(device=device, use_dummy=True)\n        self.hider = HiderNetwork(cover_channels=3, secret_channels=1).to(device)\n        self.revealer = RevealerNetwork(stego_channels=3, secret_channels=1).to(device)\n\n    def hide_video(self, video_path, output_image_path):\n        print(f\"1. Extracting frames from {video_path}...\")\n        frames = (\n            extract_frames(video_path, max_frames=16, resize_dim=(64, 64))\n            .unsqueeze(0)\n            .to(self.device)\n        )\n        print(\"2. Compressing video into latent vector...\")\n        with torch.no_grad():\n            _, latent = self.autoencoder(frames)\n        print(\"3. Encrypting latent vector using AES...\")\n        ciphertext, metadata = self.encryptor.encrypt(latent[0])\n        print(\"4. Generating Cover Image (256x256)...\")\n        cover_image = (\n            self.generator.generate_cover(size=(256, 256)).unsqueeze(0).to(self.device)\n        )\n        print(\"5. Packing encrypted data into spatial tensor...\")\n        spatial_secret = format_secret_for_hiding(\n            ciphertext_to_bits(ciphertext, 65536).unsqueeze(0).to(self.device),\n            (1, 1, 256, 256),\n        )\n        print(\"6. Embedding secret into Cover Image...\")\n        with torch.no_grad():\n            stego_image = self.hider(cover_image, spatial_secret)\n        print(f\"7. Saving Stego Image to {output_image_path}...\")\n        save_image(stego_image[0], output_image_path)\n        return metadata, len(ciphertext)\n\n    def extract_video(self, stego_image_path, metadata, cipher_len, output_video_path):\n        from torchvision.io import read_image\n\n        print(f\"1. Loading Stego Image from {stego_image_path}...\")\n        stego_image = (\n            (read_image(stego_image_path).float() / 255.0).unsqueeze(0).to(self.device)\n        )\n        print(\"2. Extracting spatial data...\")\n        with torch.no_grad():\n            secret_pred_spatial = self.revealer(stego_image)\n        print(\"3. Reconstructing bit stream...\")\n        bit_tensor = extract_secret_from_prediction(secret_pred_spatial, cipher_len * 8)\n        print(\"4. Repacking bits to ciphertext...\")\n        recovered_ciphertext = bits_to_ciphertext(bit_tensor[0], cipher_len)\n        print(\"5. Decrypting latent vector using AES...\")\n        try:\n            recovered_latent = self.encryptor.decrypt(\n                recovered_ciphertext, metadata, self.device\n            )\n            print(\"Decryption successful!\")\n        except Exception:\n            print(\n                \"Decryption failed! Networks untrained (Invalid AES Tag error expected during testing).\"\n            )\n            print(\n                \"Using a random fallback latent vector to demonstrate the rest of the pipeline...\"\n            )\n            recovered_latent = torch.rand(metadata[\"shape\"]).to(self.device)\n\n        print(\"6. Reconstructing video frames...\")\n        with torch.no_grad():\n            reconstructed_frames = self.autoencoder.decoder(\n                recovered_latent.unsqueeze(0)\n            )\n        print(f\"7. Saving Reconstructed Video to {output_video_path}...\")\n        compile_video(reconstructed_frames[0], output_video_path, fps=15)\n        print(\"--- Decode Complete ---\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    os.makedirs(\"../data\", exist_ok=True)\n    os.makedirs(\"../models\", exist_ok=True)\n    dummy_video_path = \"../data/dummy_video.mp4\"\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out = cv2.VideoWriter(dummy_video_path, fourcc, 15, (64, 64))\n    for _ in range(16):\n        out.write(np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8))\n    out.release()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    pipeline = SteganoPipeline(device=device)\n\n    stego_img_path = \"../data/stego_output.png\"\n    recon_video_path = \"../data/reconstructed_video.mp4\"\n\n    metadata, cipher_length = pipeline.hide_video(dummy_video_path, stego_img_path)\n    pipeline.extract_video(stego_img_path, metadata, cipher_length, recon_video_path)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}